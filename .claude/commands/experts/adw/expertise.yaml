overview:
  description: "AI Developer Workflows - Automated SDLC orchestration with isolated worktrees and GitHub integration"
  purpose: "Execute complete software development lifecycle through composable workflow scripts"
  last_updated: "2026-02-04T20:30:00"
  total_files: 39
  key_files:
    - "adws/adw_sdlc_iso.py"
    - "adws/adw_sdlc_zte_iso.py"
    - "adws/adw_modules/state.py"
    - "adws/adw_modules/workflow_ops.py"
    - "adws/adw_modules/agent.py"
    - "adws/adw_modules/github.py"
    - "adws/adw_modules/git_ops.py"
    - "adws/adw_modules/worktree_ops.py"

architecture:
  pattern: "Composable Workflow Architecture"
  layers:
    workflows:
      path: "adws/adw_*_iso.py"
      purpose: "Isolated workflow orchestration scripts - each runs in dedicated worktree"
      count: 14
      key_workflows:
        - adw_sdlc_iso.py: "Complete SDLC - plan → build → test → review → document"
        - adw_sdlc_zte_iso.py: "Zero Touch Execution - fully automated SDLC"
        - adw_plan_iso.py: "Planning phase with issue classification and spec generation"
        - adw_build_iso.py: "Implementation phase with TAC-10 build_w_report integration"
        - adw_test_iso.py: "Testing phase with E2E test execution and resolution"
        - adw_review_iso.py: "Review phase with code quality checks"
        - adw_document_iso.py: "Documentation generation phase"
        - adw_patch_iso.py: "Quick patch workflow for simple fixes"
        - adw_ship_iso.py: "PR creation and merging workflow"
        - adw_plan_build_iso.py: "Composite: plan + build only"
        - adw_plan_build_test_iso.py: "Composite: plan + build + test"
        - adw_plan_build_test_review_iso.py: "Composite: plan + build + test + review"
        - adw_plan_build_review_iso.py: "Composite: plan + build + review (skip test)"
        - adw_plan_build_document_iso.py: "Composite: plan + build + document"

    modules:
      path: "adws/adw_modules/"
      purpose: "Reusable infrastructure and utilities"
      count: 12
      key_modules:
        - state.py: "Persistent state management via ADWState class"
        - workflow_ops.py: "Shared operations (ADW ID management, doc detection)"
        - agent.py: "Claude Code agent execution with model selection"
        - github.py: "GitHub API operations (issues, comments, PRs)"
        - git_ops.py: "Git operations (branching, merging, status)"
        - worktree_ops.py: "Git worktree management for isolation"
        - data_types.py: "Pydantic models and SlashCommand type definitions (includes TAC-13 expert commands)"
        - tool_sequencer.py: "Tool call sequencing for multi-step operations"
        - utils.py: "Logging, parsing, and utility functions"
        - r2_uploader.py: "Cloudflare R2 uploader for context bundles"
        - adw_agent_sdk.py: "Abstract typed layer for Claude Agent SDK with Pydantic models (hooks, messages, sessions)"

    schema:
      path: "adws/schema/"
      purpose: "SQLite database schema for orchestrator state tracking"
      files:
        - README.md: "Database schema documentation and usage guide"
        - schema_orchestrator.sql: "Main schema definition with tables for agents, prompts, logs"
        - migrations/001_initial.sql: "Initial migration script for schema v0.8.0"
      description: "Zero-config SQLite database for persistent agent workflow tracking (Task 6)"

    triggers:
      path: "adws/adw_triggers/"
      purpose: "Event-driven workflow triggers"
      count: 5
      types:
        - trigger_cron.py: "Time-based workflow execution (scheduled tasks)"
        - trigger_webhook.py: "GitHub webhook handlers (issue events, PR events)"
        - trigger_issue_chain.py: "Sequential issue-based workflow chaining"
        - trigger_issue_parallel.py: "Parallel execution of multiple issue workflows"
        - trigger_plan_parallel.py: "Parallel planning phase execution"

core_implementation:

  state_management:
    location: "adws/adw_modules/state.py"
    purpose: "Persistent state tracking across workflow phases"
    line_count: "~200 lines"

    ADWState_class:
      lines: "15-150"
      purpose: "Container for workflow state with file persistence"
      state_file: "adw_state.json"

      key_fields:
        - adw_id: "Unique workflow identifier (e.g., feature_Tac_13_Task_9)"
        - issue_number: "GitHub issue number"
        - branch_name: "Git branch for this workflow"
        - plan_file: "Path to generated plan spec file"
        - issue_class: "Issue classification (feature/bug/chore/patch)"
        - worktree_path: "Path to isolated worktree"
        - model_set: "Model configuration (base/heavy)"
        - all_adws: "List of all ADW IDs in workflow chain"
        - token_tracking: "Input/output tokens, cost tracking per agent"

      key_methods:
        update:
          signature: "def update(self, **kwargs)"
          purpose: "Update state with new key-value pairs (filtered to core fields)"
          pattern: "Only allows whitelisted fields to prevent state pollution"

        get:
          signature: "def get(self, key: str, default=None)"
          purpose: "Retrieve value from state with optional default"

        append_adw_id:
          signature: "def append_adw_id(self, adw_id: str)"
          purpose: "Track ADW ID chain for multi-workflow orchestration"

        accumulate_tokens:
          signature: "def accumulate_tokens(self, agent_name: str, token_usage: TokenUsage)"
          purpose: "Track token usage per agent for cost monitoring"
          pattern: "Accumulates input/output tokens and cost in USD"

        load:
          signature: "def load(cls, adw_id: str) -> ADWState"
          purpose: "Load existing state from adw_state.json"
          pattern: "Returns initialized ADWState with persisted data"

        save:
          signature: "def save(self)"
          purpose: "Persist current state to adw_state.json"
          pattern: "Atomic write with validation"

    state_lifecycle:
      initialization: "ensure_adw_id() creates state file with adw_id"
      updates: "Each workflow phase calls state.update() with phase-specific data"
      persistence: "state.save() after each phase completion"
      handoff: "Next phase loads state via ADWState.load(adw_id)"

  workflow_orchestration:
    location: "adws/adw_modules/workflow_ops.py"
    purpose: "Shared workflow operations and utilities"
    line_count: "~1684 lines"

    key_functions:
      consult_expert:
        signature: "def consult_expert(domain: str, question: str, adw_id: str, logger: logging.Logger, working_dir: Optional[str]) -> AgentPromptResponse"
        lines: "1459-1498"
        purpose: "REUSE phase of Act → Learn → Reuse loop - consult domain expert using expertise.yaml"
        logic: |
          1. Build slash command: /experts:{domain}:question
          2. Execute via AgentTemplateRequest
          3. Return expert answer in AgentPromptResponse
          4. Log success/failure
        integration: "Called by plan, build, review, document phases when --use-experts enabled"
        domains: ["adw", "cli", "commands"]

      improve_expert_knowledge:
        signature: "def improve_expert_knowledge(domain: str, check_git_diff: bool, focus_area: Optional[str], adw_id: str, logger: logging.Logger, working_dir: Optional[str]) -> AgentPromptResponse"
        lines: "1501-1558"
        purpose: "LEARN phase of Act → Learn → Reuse loop - update expertise.yaml based on code changes"
        logic: |
          1. Build args: [check_git_diff, focus_area]
          2. Execute /experts:{domain}:self-improve
          3. Return self-improve report
          4. Log completion status
        integration: "Called by phase workflows when --expert-learn enabled"
        focus_areas: ["planning_phase", "review_phase", "build_phase", "document_phase"]

      extract_file_references_from_issue:
        signature: "def extract_file_references_from_issue(issue: GitHubIssue, logger: logging.Logger, working_dir: Optional[str]) -> dict[str, str]"
        lines: "1561-1659"
        purpose: "Auto-detect and load markdown files referenced in issue body and comments"
        logic: |
          1. Scan issue body and comments for file references (e.g., plan_tasks_Tac_14.md)
          2. Search in multiple locations: specs/, ai_docs/, ai_docs/doc/, root
          3. Load file content if found
          4. Return dict mapping file paths to content
        patterns:
          - "*.md file references in issue text"
          - "Support paths: specs/*, ai_docs/*, ai_docs/doc/*, root/*"
        integration: "Called by adw_plan_iso.py in both scout and standard planning paths"

      format_file_references_for_context:
        signature: "def format_file_references_for_context(file_references: dict[str, str]) -> str"
        lines: "1660-1684"
        purpose: "Format loaded file references as context for agent prompts"
        logic: |
          1. Build markdown section with file paths and content
          2. Format as code blocks for agent consumption
          3. Return empty string if no files
        integration: "Used by adw_plan_iso.py to append file content to planning context"

      ensure_adw_id:
        signature: "def ensure_adw_id(issue_number: str, adw_id: Optional[str]) -> str"
        lines: "~50-100"
        logic: |
          1. If adw_id provided: validate format and return
          2. If no adw_id: fetch issue from GitHub
          3. Classify issue (feature/bug/chore/patch)
          4. Generate adw_id format: {issue_class}_{adw_suffix}
          5. Initialize ADWState with new adw_id
          6. Save state and return adw_id
        integration: "Called at start of every workflow script"

      detect_relevant_docs:
        signature: "def detect_relevant_docs(issue: GitHubIssue) -> Optional[str]"
        lines: "~150-200"
        purpose: "TAC-9 integration - detect documentation topics from issue"
        logic: |
          1. Parse issue body for keyword patterns
          2. Map keywords to ai_docs topics (tac-9, tac-10, tac-12, etc.)
          3. Return comma-separated topic list
          4. Used for /load_ai_docs command
        patterns:
          - "'scout|explore|TAC-12' -> 'tac-12'"
          - "'build_w_report|parallel|TAC-10' -> 'tac-10'"
          - "'docs|documentation|TAC-9' -> 'tac-9'"

      format_issue_message:
        signature: "def format_issue_message(adw_id: str, agent_name: str, message: str, session_id: Optional[str]) -> str"
        purpose: "Format GitHub issue comments with ADW tracking"
        pattern: "[ADW-AGENTS] {adw_id}_{agent_name}: {message}"
        reason: "Prevents webhook loops, enables comment filtering"

      get_minimal_issue_json:
        signature: "def get_minimal_issue_json(issue: GitHubIssue, max_body_length: int = 2000) -> str"
        purpose: "Token optimization - truncate issue body for agent prompts"
        pattern: "Returns JSON with number, title, and truncated body"

    constants:
      AGENT_PLANNER: "sdlc_planner"
      AGENT_IMPLEMENTOR: "sdlc_implementor"
      AGENT_CLASSIFIER: "issue_classifier"
      AGENT_BRANCH_GENERATOR: "branch_generator"
      AGENT_PR_CREATOR: "pr_creator"
      AGENT_EXPERT_ADW: "adw_expert"
      AGENT_EXPERT_CLI: "cli_expert"
      AGENT_EXPERT_COMMANDS: "commands_expert"
      MAX_ISSUE_BODY_LENGTH: 2000

  agent_execution:
    location: "adws/adw_modules/agent.py"
    purpose: "Claude Code agent orchestration with model selection"
    line_count: "~920 lines"

    key_functions:
      execute_prompt:
        signature: "def execute_prompt(request: AgentPromptRequest) -> AgentPromptResponse"
        lines: "~200-300"
        logic: |
          1. Build claude command with model, cwd, and prompt
          2. Execute via subprocess with environment variables
          3. Parse JSON response from Claude Code
          4. Extract token usage statistics
          5. Return AgentPromptResponse with results and tokens
        integration: "Used by workflows for direct prompt execution"

      execute_template:
        signature: "def execute_template(request: AgentTemplateRequest) -> AgentPromptResponse"
        lines: "~350-450"
        logic: |
          1. Resolve slash command from request
          2. Select model based on model_set and command mapping
          3. Build command: claude {slash_command} {args} --model {model}
          4. Execute via subprocess
          5. Parse response and token usage
          6. Return structured response
        integration: "Primary method for workflow script execution"

      get_model_for_slash_command:
        signature: "def get_model_for_slash_command(request: AgentTemplateRequest, default: str = 'sonnet') -> str"
        purpose: "Model selection based on command complexity and model_set from ADW state"
        mapping:
          base_models:
            lightweight: "sonnet (classify, commit, test)"
            planning: "sonnet (feature, bug, chore)"
            implementation: "sonnet (implement, build_w_report)"
          heavy_models:
            lightweight: "sonnet (no change)"
            planning: "opus (complex planning)"
            implementation: "opus (complex builds)"
        pattern: "SLASH_COMMAND_MODEL_MAP dict with base/heavy variants"

    model_selection:
      base_set: "Cost-optimized - sonnet for all"
      heavy_set: "Quality-optimized - opus for planning, implementation, resolution, documentation"

    token_tracking:
      purpose: "Monitor and report token usage per agent"
      extraction: "Parse ClaudeCodeResultMessage from JSON response"
      fields:
        - total_input_tokens: "Input tokens from user and tools"
        - output_tokens: "Generated tokens from model"
        - total_cost_usd: "Estimated cost in USD"
      integration: "Passed to ADWState.accumulate_tokens()"

  github_integration:
    location: "adws/adw_modules/github.py"
    purpose: "GitHub API operations via gh CLI"
    line_count: "~500 lines"

    key_functions:
      fetch_issue:
        signature: "def fetch_issue(issue_number: str, repo_path: str) -> GitHubIssue"
        logic: |
          1. Execute: gh api repos/{repo_path}/issues/{issue_number}
          2. Parse JSON response
          3. Return typed GitHubIssue object
        integration: "Used by ensure_adw_id and planning workflows"

      make_issue_comment:
        signature: "def make_issue_comment(issue_number: str, message: str, repo_path: str)"
        logic: |
          1. Rate limit delay (5s between calls, 3s initial)
          2. Retry with exponential backoff on rate limit AND network errors
          3. Execute: gh api repos/{repo_path}/issues/{issue_number}/comments
          4. Post JSON body with comment text
        pattern: "Always includes [ADW-AGENTS] identifier"
        rate_limiting:
          - MIN_DELAY_BETWEEN_COMMENTS: "5.0 seconds"
          - INITIAL_DELAY: "3.0 seconds"
          - MAX_RETRIES: "5 attempts"
          - RETRY_BACKOFF: "10.0 seconds"

      _execute_with_retry:
        signature: "def _execute_with_retry(cmd, env, operation_name='operation')"
        lines: "57-113"
        purpose: "Execute GitHub CLI commands with retry on rate limits AND network errors"
        logic: |
          1. Execute command via subprocess
          2. Check for retryable errors in stderr
          3. Rate limit errors: wait RETRY_BACKOFF * (attempt + 1) seconds
          4. Network errors: exponential backoff 2^attempt, capped at 30s
          5. Retry up to MAX_RETRIES times
          6. Return result (success or final failure)
        network_errors_handled:
          - "connection reset by peer"
          - "connection refused / timed out"
          - "temporary failure / network unreachable"
          - "broken pipe"
          - "502 bad gateway / 503 service unavailable / 504 gateway timeout"
        integration: "Used internally by all GitHub API operations"

      fetch_issue_comments:
        signature: "def fetch_issue_comments(repo_path: str, issue_number: int) -> List[Dict]"
        purpose: "Fetch all comments for issue"
        integration: "Used for comment filtering and history"

      get_current_gh_user:
        signature: "def get_current_gh_user() -> Optional[str]"
        purpose: "Get current GitHub CLI authenticated user"
        integration: "Used for issue assignment checks"

      assign_issue_to_me:
        signature: "def assign_issue_to_me(issue_id: str) -> bool"
        logic: "Assign issue to current GitHub user"
        integration: "Used by workflows to claim issues"

      mark_issue_in_progress:
        signature: "def mark_issue_in_progress(issue_id: str) -> None"
        purpose: "Add 'in progress' label to issue"
        integration: "Called at workflow start"

      get_repo_url:
        signature: "def get_repo_url() -> str"
        logic: "Execute: git remote get-url origin"
        integration: "Extract repo path for GitHub API calls"

    constants:
      ADW_BOT_IDENTIFIER: "[ADW-AGENTS]"
      purpose: "Prevents webhook loops, enables comment filtering"

  git_operations:
    location: "adws/adw_modules/git_ops.py"
    purpose: "Git operations for branch and worktree management"
    line_count: "~300 lines"

    key_functions:
      create_feature_branch:
        signature: "def create_feature_branch(branch_name: str, base_branch: str = 'main')"
        logic: |
          1. Fetch latest from origin
          2. Create branch from base: git checkout -b {branch_name} origin/{base_branch}
          3. Handle conflicts if branch exists
        integration: "Called by planning workflows"

      switch_branch:
        signature: "def switch_branch(branch_name: str)"
        logic: "Execute: git checkout {branch_name}"
        integration: "Used for worktree branch switching"

      commit_changes:
        signature: "def commit_changes(message: str, files: List[str])"
        logic: |
          1. Stage files: git add {files}
          2. Commit: git commit -m "{message}"
        integration: "Used after build and test phases"

      push_branch:
        signature: "def push_branch(branch_name: str, force: bool = False)"
        logic: "Execute: git push origin {branch_name} [--force]"
        integration: "Used by ship workflow"

      get_git_status:
        signature: "def get_git_status() -> Dict[str, List[str]]"
        logic: |
          1. Execute: git status --porcelain
          2. Parse output into staged/unstaged/untracked
          3. Return categorized file lists
        integration: "Used for state verification"

  worktree_management:
    location: "adws/adw_modules/worktree_ops.py"
    purpose: "Git worktree operations for workflow isolation"
    line_count: "~154 lines"

    key_functions:
      create_worktree:
        signature: "def create_worktree(adw_id: str, branch_name: str, logger: logging.Logger) -> Tuple[str, Optional[str]]"
        logic: |
          1. Ensure worktree directory: trees/{branch_name}
          2. Create worktree: git worktree add {path} -b {branch}
          3. Return (worktree_path, error_message)
        pattern: "Worktrees enable parallel workflow execution"

      validate_worktree:
        signature: "def validate_worktree(adw_id: str, state: ADWState) -> Tuple[bool, Optional[str]]"
        purpose: "Validate worktree exists and is in correct state"
        pattern: "Checks worktree path exists and matches state"

      remove_worktree:
        signature: "def remove_worktree(adw_id: str, logger: logging.Logger) -> Tuple[bool, Optional[str]]"
        logic: |
          1. Execute: git worktree remove {path}
          2. Cleanup worktree directory
          3. Return (success, error_message)
        integration: "Called by ship workflow after merge"

      get_worktree_path:
        signature: "def get_worktree_path(adw_id: str) -> str"
        purpose: "Get worktree path for ADW ID"
        pattern: "Returns trees/{adw_id} path"

    worktree_isolation_pattern:
      purpose: "Enable parallel workflow execution without conflicts"
      structure: |
        repo_root/
        ├── .git/           (main repo)
        ├── adws/           (workflow scripts - shared)
        └── trees/          (isolated worktrees)
            ├── feature-branch-1/
            ├── feature-branch-2/
            └── bugfix-branch-1/
      benefits:
        - "Each workflow has independent working directory"
        - "No git state conflicts between parallel workflows"
        - "Safe concurrent execution of multiple ADWs"
        - "Isolated port assignments for test servers"

key_operations:

  sdlc_workflow_execution:
    command: "uv run adws/adw_sdlc_iso.py <issue-number> [adw-id]"
    description: "Execute complete SDLC pipeline from issue to documentation"
    expert_system_flags:
      use_experts: "Enable expert consultation (default: on, opt-out: --no-experts)"
      expert_learn: "Enable expert learning after phases (default: on, opt-out: --no-expert-learn)"
      integration: "SDLC orchestrator passes flags to all phase workflows"
    phases:
      1_plan:
        script: "adw_plan_iso.py"
        actions:
          - "Classify issue (feature/bug/chore/patch)"
          - "Generate branch name"
          - "Create feature branch"
          - "TAC-13: Consult ADW expert for planning guidance (if --use-experts)"
          - "Extract file references from issue body and comments (auto-load .md files)"
          - "Load referenced files and append to planning context"
          - "Generate spec file via /feature|/bug|/chore|/patch with file context"
          - "Save plan_file to state"
          - "TAC-13: Learn from planning phase (if --expert-learn)"

      2_build:
        script: "adw_build_iso.py"
        actions:
          - "Create worktree for isolated execution"
          - "TAC-13: Consult ADW expert for build patterns (if --use-experts)"
          - "Execute /build_w_report with plan_file"
          - "Generate YAML build report"
          - "Commit changes with /commit"
          - "Update state with build results"
          - "TAC-13: Learn from build phase (if --expert-learn)"

      3_test:
        script: "adw_test_iso.py"
        actions:
          - "Execute /test for unit tests"
          - "Execute /test_e2e for E2E tests (unless --skip-e2e)"
          - "If failures: /resolve_failed_test or /resolve_failed_e2e_test"
          - "Retry until tests pass (unless --skip-resolution)"
          - "Commit fixes with /commit"

      4_review:
        script: "adw_review_iso.py"
        actions:
          - "TAC-13: Consult ADW expert for review criteria (if --use-experts)"
          - "Execute /review with plan_file"
          - "Check code quality and adherence to plan"
          - "Generate review report"
          - "Commit review fixes if needed"
          - "TAC-13: Learn from review phase (if --expert-learn)"

      5_document:
        script: "adw_document_iso.py"
        actions:
          - "TAC-13: Consult ADW expert for documentation patterns (if --use-experts)"
          - "Execute /document for feature documentation"
          - "Generate user-facing documentation"
          - "Commit documentation with /commit"
          - "TAC-13: Learn from documentation phase (if --expert-learn)"

    state_transitions:
      - "plan → build: pass adw_id and plan_file"
      - "build → test: pass adw_id and worktree_path"
      - "test → review: pass adw_id"
      - "review → document: pass adw_id"

    options:
      load_docs: "Manual documentation override (TAC-9 hybrid)"
      skip_e2e: "Skip E2E test execution"
      skip_resolution: "Skip test failure auto-resolution"

  zero_touch_execution:
    command: "uv run adws/adw_sdlc_zte_iso.py <issue-number>"
    description: "Fully automated SDLC with automatic PR creation and merge"
    workflow: |
      1. Run adw_sdlc_iso.py (plan → build → test → review → document)
      2. Execute adw_ship_iso.py (PR creation)
      3. Auto-merge PR if all checks pass
      4. Close issue with completion comment
      5. Cleanup worktree
    integration: "Webhook trigger for 'zte' label on issues"

  patch_workflow:
    command: "uv run adws/adw_patch_iso.py <issue-number> [adw-id]"
    description: "Quick patch workflow for simple fixes"
    workflow: |
      1. Generate patch spec via /patch
      2. Implement via /implement
      3. Run tests via /test
      4. Commit with /commit
    use_cases:
      - "Simple bug fixes (< 50 lines)"
      - "Documentation updates"
      - "Configuration changes"
      - "Dependency updates"

  ship_workflow:
    command: "uv run adws/adw_ship_iso.py <adw-id>"
    description: "Create PR and optionally merge"
    workflow: |
      1. Load state from adw_id
      2. Push branch to origin
      3. Execute /pull_request slash command to create PR (via Claude Code agent)
      4. Post PR link to issue
      5. If auto_merge: merge PR and cleanup worktree
    options:
      auto_merge: "Automatically merge PR after creation"
      cleanup: "Remove worktree after successful merge"
    note: "PR creation uses /pull_request command, not direct GitHub API"

  state_persistence:
    file: "adw_state.json"
    format: "JSON with typed validation via ADWStateData"
    location: "Repository root"
    lifecycle:
      1_initialization: "ensure_adw_id() creates state with adw_id"
      2_updates: "Each workflow phase updates state with phase data"
      3_persistence: "state.save() writes to adw_state.json"
      4_handoff: "Next phase loads via ADWState.load(adw_id)"
      5_cleanup: "Ship workflow archives state after merge"

    typical_state:
      adw_id: "feature_Tac_13_Task_9"
      issue_number: "571"
      branch_name: "feature-issue-571-adw-feature_Tac_13_Task_9"
      plan_file: "specs/issue-571-feature_Tac_13_Task_9-spec.md"
      issue_class: "feature"
      worktree_path: "trees/feature-issue-571-adw-feature_Tac_13_Task_9"
      model_set: "base"
      all_adws: ["feature_Tac_13_Task_9"]
      total_input_tokens: 45000
      total_output_tokens: 8500
      total_cost_usd: 1.23

  documentation_loading:
    integration: "TAC-9 automatic documentation detection"
    mechanism: "detect_relevant_docs() parses issue body for keywords"
    mapping:
      keywords_to_topics:
        - "'scout|explore|TAC-12' -> 'tac-12'"
        - "'build_w_report|parallel|TAC-10' -> 'tac-10'"
        - "'docs|documentation|TAC-9' -> 'tac-9'"
        - "'agent|expert|TAC-13' -> 'tac-13'"
        - "'hook|integration|TAC-5' -> 'tac-5'"
    command: "/load_ai_docs {topics}"
    fallback: "--load-docs flag for manual override"

  model_selection_strategy:
    base_set: "Cost-optimized - sonnet default for dev iterations"
    heavy_set: "Quality-optimized - opus for planning/build/resolution in prod"
    pattern: "Lightweight ops (classify, commit, test) always use sonnet"

best_practices:

  workflow_isolation:
    principle: "Each ADW executes in dedicated worktree"
    implementation: |
      1. Create worktree at trees/{branch-name}
      2. Execute workflow in worktree directory
      3. All file operations scoped to worktree
      4. Cleanup worktree after merge
    benefits:
      - "No conflicts between parallel workflows"
      - "Independent working directories"
      - "Safe concurrent execution"
      - "Isolated port assignments"

  state_management:
    principle: "Single source of truth via adw_state.json"
    patterns:
      - "Initialize state early with ensure_adw_id()"
      - "Update state after each phase completion"
      - "Always save state before phase exit"
      - "Load state at start of dependent phases"
      - "Never mutate state without save()"
    validation: "Use ADWStateData Pydantic model for type safety"

  github_integration:
    principle: "Rate limiting and bot identification"
    patterns:
      - "Always prefix comments with [ADW-AGENTS]"
      - "Enforce 5s delay between API calls"
      - "Retry with exponential backoff on rate limits"
      - "Filter bot comments to prevent context pollution"
      - "Use minimal issue JSON to reduce token usage"
    anti_patterns:
      - "Don't spam issue comments"
      - "Don't post sensitive data (tokens, credentials)"
      - "Don't skip bot identifier (causes webhook loops)"

  agent_execution:
    principle: "Model selection based on task complexity"
    patterns:
      - "Use base model_set for development iterations"
      - "Use heavy model_set for production workflows"
      - "Track token usage per agent for cost monitoring"
      - "Pass cwd parameter to execute in worktree"
      - "Always validate AgentPromptResponse for errors"
    error_handling:
      - "Check response.error for agent failures"
      - "Retry with heavy model on complex task failures"
      - "Log token usage even on failures"

  composable_workflows:
    principle: "Workflows compose via ADW ID and state handoff"
    patterns:
      - "SDLC workflows call phase workflows sequentially"
      - "Each phase is independently executable"
      - "State persistence enables workflow chaining"
      - "ADW ID serves as workflow identifier"
    example: |
      # Run individual phases
      uv run adws/adw_plan_iso.py 571 feature_Tac_13_Task_9
      uv run adws/adw_build_iso.py feature_Tac_13_Task_9
      uv run adws/adw_test_iso.py feature_Tac_13_Task_9

      # Or run complete SDLC
      uv run adws/adw_sdlc_iso.py 571

  documentation_integration:
    principle: "TAC-9 automatic documentation detection"
    patterns:
      - "Auto-detect docs from issue keywords"
      - "Support manual override with --load-docs"
      - "Load docs at planning phase for context"
      - "Use /load_ai_docs command for agent context"
    keyword_detection:
      - "Parse issue title and body"
      - "Map keywords to ai_docs topics"
      - "Pass topics to planning agent"

  error_recovery:
    principle: "Automatic failure resolution with retries"
    patterns:
      - "Test failures trigger /resolve_failed_test"
      - "E2E failures trigger /resolve_failed_e2e_test"
      - "Retry up to 3 times with different strategies"
      - "Use heavy model for resolution attempts"
      - "Skip resolution with --skip-resolution flag"
    escalation:
      - "After 3 failures: post comment and exit"
      - "Manual intervention required"
      - "State preserved for later resumption"

  token_optimization:
    principle: "Minimize token usage without sacrificing quality"
    patterns:
      - "Truncate issue body to 2000 chars"
      - "Use minimal issue JSON (number, title, body)"
      - "Filter bot comments from context"
      - "Track token usage per agent"
      - "Use haiku model for simple operations"
    monitoring:
      - "Log token usage after each agent execution"
      - "Accumulate tokens in ADWState"
      - "Report total cost in PR comments"

  branch_naming:
    principle: "Consistent branch naming for automation"
    format: "{issue_class}-issue-{issue_number}-adw-{adw_id}"
    examples:
      - "feature-issue-571-adw-feature_Tac_13_Task_9"
      - "bug-issue-234-adw-bug_api_timeout"
      - "patch-issue-789-adw-patch_typo_fix"
    benefits:
      - "Easy identification of ADW branches"
      - "Automated cleanup via branch patterns"
      - "GitHub integration via branch naming"

  worktree_cleanup:
    principle: "Clean up worktrees after successful merge"
    patterns:
      - "Ship workflow removes worktree post-merge"
      - "Manual cleanup: git worktree remove trees/{branch}"
      - "List worktrees: git worktree list"
    automation:
      - "ZTE workflow auto-cleans on success"
      - "Failed workflows preserve worktree for debugging"

data_flow:

  state_lifecycle:
    initialization:
      trigger: "ensure_adw_id() at workflow start"
      action: "Create adw_state.json with adw_id"
      data: "{adw_id, issue_number, model_set}"

    planning_phase:
      input: "adw_id, issue_number"
      process: |
        1. Classify issue
        2. Generate branch name
        3. Create feature branch
        4. Generate spec via slash command
        5. Update state
      output: "plan_file path"
      state_updates: "{branch_name, plan_file, issue_class}"

    build_phase:
      input: "adw_id, plan_file"
      process: |
        1. Create worktree
        2. Execute /build_w_report with plan_file
        3. Generate build report YAML
        4. Commit changes
        5. Update state
      output: "build_report.yml"
      state_updates: "{worktree_path, build_completed: true}"

    test_phase:
      input: "adw_id, worktree_path"
      process: |
        1. Execute /test
        2. Execute /test_e2e (unless --skip-e2e)
        3. If failures: execute resolution commands
        4. Retry until pass or max retries
        5. Commit fixes
        6. Update state
      output: "test_results"
      state_updates: "{tests_passed: true, test_attempts: N}"

    review_phase:
      input: "adw_id, plan_file"
      process: |
        1. Execute /review with plan_file
        2. Check code quality
        3. Commit fixes if needed
        4. Update state
      output: "review_report"
      state_updates: "{review_completed: true}"

    document_phase:
      input: "adw_id"
      process: |
        1. Execute /document
        2. Generate user docs
        3. Commit documentation
        4. Update state
      output: "documentation files"
      state_updates: "{documentation_completed: true}"

    ship_phase:
      input: "adw_id"
      process: |
        1. Push branch to origin
        2. Execute /pull_request
        3. Create PR via GitHub API
        4. Post PR link to issue
        5. Optionally merge PR
        6. Cleanup worktree
      output: "PR URL"
      state_updates: "{pr_url, shipped: true}"

  token_accumulation:
    tracking: "Per-agent token usage accumulated in ADWState"
    flow: |
      1. Agent executes (via execute_template)
      2. Parse token usage from response
      3. Call state.accumulate_tokens(agent_name, token_usage)
      4. Update state totals (input, output, cost)
      5. Append agent record to state
      6. Save state
    reporting: "Generate token summary for PR comments"

integrations:

  tac_9_documentation:
    feature: "Automatic AI documentation detection and loading"
    mechanism: "detect_relevant_docs() parses issue keywords"
    commands:
      - "/load_ai_docs {topics}"
    integration_point: "Planning phase (adw_plan_iso.py)"
    workflow: |
      1. Fetch issue from GitHub
      2. Detect relevant docs via keyword matching
      3. Load docs with /load_ai_docs command
      4. Agent uses docs for context in planning
    fallback: "--load-docs flag for manual override"

  tac_10_parallel_build:
    feature: "Parallel build execution with detailed YAML reporting"
    commands:
      - "/build_w_report"
      - "/build_in_parallel"
    integration_point: "Build phase (adw_build_iso.py)"
    workflow: |
      1. Execute /build_w_report with plan_file
      2. Agent builds implementation in parallel
      3. Generate build_report.yml with file changes
      4. Commit all changes
    benefits:
      - "Faster implementation via parallelization"
      - "Detailed change tracking in YAML"
      - "Structured reporting for review"

  tac_12_scout:
    feature: "Parallel codebase exploration"
    commands:
      - "/scout {query} [scale]"
    integration_point: "Planning and review phases"
    workflow: |
      1. Execute /scout for codebase exploration
      2. Parallel agents search code patterns
      3. Aggregate findings
      4. Use findings for planning or review
    scales:
      - "quick: 2-3 scout agents"
      - "medium: 4-6 scout agents"
      - "thorough: 8+ scout agents"

  tac_13_agent_experts:
    feature: "Domain expert agents with expertise files"
    experts:
      - cli_expert: "CLI and code generation expertise"
      - adw_expert: "ADW workflow expertise (this file)"
      - commands_expert: "Slash commands expertise"
    commands:
      - "/experts:adw:question"
      - "/experts:adw:self-improve"
      - "/experts:cli:question"
      - "/experts:cli:self-improve"
      - "/experts:commands:question"
      - "/experts:commands:self-improve"
    integration_point: "All workflows via expert consultation"
    workflow: |
      1. Agent consults expertise file for guidance
      2. Makes decisions based on documented patterns
      3. Self-improve updates expertise after learning
    benefits:
      - "Reduced context needed per execution"
      - "Consistent decision-making"
      - "Accumulated domain knowledge"

    act_learn_reuse_implementation:
      reuse_phase:
        function: "consult_expert(domain, question, adw_id, logger, working_dir)"
        location: "adws/adw_modules/workflow_ops.py:1450-1488"
        pattern: |
          1. Phase workflow calls consult_expert() with domain-specific question
          2. Function executes /experts:{domain}:question via agent
          3. Expert responds using expertise.yaml knowledge
          4. Response stored in state and tokens accumulated
        example_questions:
          planning: "What planning patterns should I apply from ADW expertise?"
          review: "What review criteria should I apply?"
          build: "What build patterns should I follow?"
          document: "What documentation patterns should I use?"

      learn_phase:
        function: "improve_expert_knowledge(domain, check_git_diff, focus_area, adw_id, logger, working_dir)"
        location: "adws/adw_modules/workflow_ops.py:1491-1541"
        pattern: |
          1. Phase workflow calls improve_expert_knowledge() after completion
          2. Function executes /experts:{domain}:self-improve with focus_area
          3. Self-improve validates expertise against codebase
          4. Updates expertise.yaml with new patterns learned
          5. Response tokens accumulated in state
        focus_areas:
          planning_phase: "adw_plan_iso.py"
          review_phase: "adw_review_iso.py"
          build_phase: "adw_build_iso.py"
          document_phase: "adw_document_iso.py"

      orchestrator_integration:
        location: "adws/adw_sdlc_iso.py:46-242"
        flags:
          use_experts: "--use-experts flag passed to phase workflows (default on)"
          expert_learn: "--expert-learn flag passed to phase workflows (default on)"
          opt_out: "--no-experts and --no-expert-learn for disabling"
        pattern: |
          1. SDLC orchestrator checks for --no-experts and --no-expert-learn flags
          2. Sets use_experts and expert_learn booleans (default True)
          3. Passes flags to each phase subprocess command
          4. Phase workflows conditionally call expert functions based on flags

recent_changes:
  - date: "2026-02-04"
    description: "Added SQLite schema implementation for orchestrator state tracking (TAC-14 Task 6)"
    files:
      - "adws/schema/README.md"
      - "adws/schema/schema_orchestrator.sql"
      - "adws/schema/migrations/001_initial.sql"
    impact: |
      SQLite schema (commit 266666d):
      - Added schema/ directory with orchestrator database schema
      - Defines tables: orchestrator_agents, agents, prompts, agent_logs, system_logs
      - Zero-config pattern with auto-initialization
      - Schema v0.8.0 for persistent state tracking
      - Supports async via aiosqlite
      - Database location: adws/schema/orchestrator.db (gitignored)

  - date: "2026-02-04"
    description: "ZTE workflow now auto-loads file references from issue (hybrid detection)"
    files:
      - "adws/adw_sdlc_zte_iso.py"
    impact: |
      ZTE enhancement (commit 2d0387f):
      - ZTE workflow calls extract_file_references_from_issue()
      - Auto-loads .md files referenced in issue body/comments
      - Improves planning context for fully automated workflows

  - date: "2026-02-04"
    description: "Added Agent SDK module (adw_agent_sdk.py, 1655 lines) with typed Pydantic models for Claude Agent SDK"
    files: ["adws/adw_modules/adw_agent_sdk.py"]
    impact: "Abstract layer for hooks, messages, sessions - not yet integrated with workflows"

  - date: "2026-02-04"
    description: "File reference auto-loading and enhanced GitHub retry logic"
    files: ["adws/adw_modules/workflow_ops.py", "adws/adw_modules/github.py", "adws/adw_plan_iso.py"]
    impact: "Auto-loads .md files from issue. GitHub retry handles network errors. workflow_ops: 1684 lines"

expert_system_integration:
  adw_expert:
    expertise_file: ".claude/commands/experts/adw/expertise.yaml"
    question_prompt: ".claude/commands/experts/adw/question.md"
    self_improve_prompt: ".claude/commands/experts/adw/self-improve.md"

    act_learn_reuse_loop:
      act: "ADW expert answers workflow questions using expertise file"
      learn: "Self-improve validates expertise against codebase, updates file"
      reuse: "Updated expertise used for future workflow decisions"

    expertise_maintenance:
      max_lines: 1000
      update_trigger: "Manual (/experts:adw:self-improve) or git hook"
      validation: "7-phase workflow ensures accuracy"
      preservation: "FileAction.CREATE with skip_if_exists prevents overwrite"
