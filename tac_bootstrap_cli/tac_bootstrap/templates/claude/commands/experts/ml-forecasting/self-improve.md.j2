---
allowed-tools: Read, Grep, Glob, Bash, Edit, Write, TodoWrite
description: Self-improve ML Forecasting expertise by validating against codebase implementation
argument-hint: [check_git_diff (true/false)] [focus_area (optional)]
---

# Purpose

You maintain the ML Forecasting expert system's expertise accuracy by comparing the existing expertise file against the actual codebase implementation. Follow the `Workflow` section to detect and remedy any differences, missing pieces, or outdated information, ensuring the expertise file remains a powerful **mental model** and accurate memory reference for ML forecasting tasks.

## Variables

CHECK_GIT_DIFF: $1 default to false if not specified
FOCUS_AREA: $2 default to empty string
EXPERTISE_FILE: .claude/commands/experts/ml-forecasting/expertise.yaml
MAX_LINES: 1000

## Instructions

- This is a self-improvement workflow to keep ML Forecasting expertise synchronized with the actual codebase
- Think of the expertise file as your **mental model** and memory reference for all ML forecasting functionality
- Always validate expertise against real implementation, not assumptions
- Focus exclusively on ML/Forecasting functionality (models, features, metrics, pipelines)
- If FOCUS_AREA is provided, prioritize validation and updates for that specific area
- Maintain the YAML structure of the expertise file
- Enforce strict line limit of 1000 lines maximum
- Prioritize actionable, high-value expertise over verbose documentation
- When trimming, remove least critical information that won't impact expert performance
- Git diff checking is optional and controlled by the CHECK_GIT_DIFF variable
- Be thorough in validation but concise in documentation
- Write as a principal engineer that writes CLEARLY and CONCISELY for future engineers.
- Keep in mind, after your thorough search, there may be nothing to be done - this is perfectly acceptable.

## Workflow

1. **Check Git Diff (Conditional)**
   - If CHECK_GIT_DIFF is "true", run `git diff` to identify recent changes to ML-related files
   - If changes detected, note them for targeted validation in step 3
   - If CHECK_GIT_DIFF is "false", skip this step

2. **Read Current Expertise**
   - Read the entire EXPERTISE_FILE to understand current documented expertise
   - Identify key sections: overview, frameworks, feature_engineering, evaluation, deployment
   - Note any areas that seem outdated or incomplete

3. **Validate Against Codebase**
   - Read the EXPERTISE_FILE to identify which files are documented as key implementation files
   - Read those files to understand current implementation
   - Compare documented expertise against actual code
   - If FOCUS_AREA is provided, prioritize validation of that specific area

4. **Identify Discrepancies**
   - List all differences found

5. **Update Expertise File**
   - Remedy all identified discrepancies by updating EXPERTISE_FILE
   - Maintain YAML structure and formatting

6. **Enforce Line Limit**
   - Run: `wc -l .claude/commands/experts/ml-forecasting/expertise.yaml`
   - If line count > MAX_LINES: trim least critical sections
   - REPEAT until line count â‰¤ MAX_LINES

7. **Validation Check**
   - Read the updated EXPERTISE_FILE
   - Validate YAML syntax: `python3 -c "import yaml; yaml.safe_load(open('.claude/commands/experts/ml-forecasting/expertise.yaml'))"`

## Report

### Summary
- Brief overview of self-improvement execution
- Total discrepancies found and remedied
- Final line count vs MAX_LINES

### Discrepancies Found
- List each discrepancy

### Updates Made
- Concise list of all updates

### Line Limit Enforcement
- Initial and final line counts

### Validation Results
- Confirm expertise is present and YAML is valid

### Codebase References
- List of files validated against
