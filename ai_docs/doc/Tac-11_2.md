## Transcripción: QA Agent → Try Copywriter → Micro SDLC → Ultra Stream + Cuándo usar custom vs out-of-the-box

### QA Agent (terminal + contexto avanzado + gobernanza)

**00:00**  
Our next powerful agent. So, what's a QA agent? How does this work? What's going on here?  
This agent kicks things up a notch. Let's fire it up: `uvr qa_agent.py`.

**00:20**  
We are back in the terminal interface. This agent is specialized and veers into advanced context engineering.  
This agent is an **expert** (see “elite context engineering extended lesson”).  
It’s a **codebase question & answer system**—useful for:
- understanding the codebase  
- onboarding new engineers  
- helping engineers plan/build new features

**00:52**  
“List all your tools from your system prompt.”  
We see a new message type: **system message**.  
It shows tools available right away, and shows the MCP server.  
This agent also uses the **Firecrawl MCP server**.

**01:20**  
It has ~13 tools (bash, glob, grep, read, web search, firecrawl search, etc.).  
It probably doesn’t need both web search systems, but they’re used as fallbacks if Firecrawl isn’t set up.

**01:38**  
“Summarize the QA agent; think hard.”  
We parse a thinking response block and perform **tool auditing**.  
This agent has **governance and permission checks**.

**02:03**  
It reads the file, understands it, and summarizes: tool management, rich UI, SDK integration, security features.  
This is valuable for you, new engineers, and feature planning.

---

### QA Agent internals: Options + System Prompt + MCP config + Hooks (permission system)

**02:26**  
Open QA agent; ~570 lines `main`. No custom tools (tooling is via enabled toolset).  
Search for `cloud_code_options` → the most important block: **options** (declares most of the core four).

**02:58**  
New additions:
- hooks  
- resume (continue conversation)  
- explicit model  
- disallow some tools / allow others  
- custom system prompt

**03:10**  
System prompt: consistent codebase structure.  
Your system prompt is the **law**.  
You must be sure you want it applied on every execution.

**03:49**  
Purpose: understand/answer questions about functionality using parallel search + intelligent analysis.  
System prompt includes:
- instructions (multiple sections)  
- current working directory info  
- **agent examples** (shows exactly how the agent should respond)

**04:27**  
Allowed tools: not creating an MCP server manually, but **extending via MCP config path**.  
References Firecrawl MCP server (noted as ~6K tokens consumed).  
This shows custom agents can tap into the broader MCP ecosystem.

**05:07**  
Allowed tools are codebase-search tools; **no writing**—it’s a QA agent.  
Core four: context / model / prompt / tools.

**05:36**  
Hooks: leverage Cloud Code hooks to tap into the **permission system**.  
We block environment variable files with matchers:
- pre-tool use  
- post-tool use

**06:02**  
In `pre-tool use`, when matching the `read` command: block env files.  
Test: “read the env file” → blocked access, returns a concise reason.  
Warnings for other files are also possible.  
Hooks are essential for governance/control/permissions.

---

## Try Copywriter (agents in UI + multi-variation output + no tools)

**07:11**  
Move to richer applications embedding agents into user experiences: **Try Copywriter**.  
Now we see a structure: backend, frontend, prompts.

**07:34**  
We type “hi” and immediately see:
- one agent generates a response with **six variations**  
- a multi-variation copywriter agent

**07:52**  
You can drag & drop a file to load context (script from elite context engineering lesson).  
Prompt: “Write a blog post title (30–60 chars).”  
One prompt → six variations.

**08:20**  
This is powerful because writing/creating is filtering many variations/futures/possibilities.  
You can pick how many versions to generate, and request copy:
- “Four section blog outline, one sentence each”  
- six versions in markdown

**09:31**  
We also track cost, which is reasonable given six versions.

**09:34**  
Try Copy Agent internals:
- Claude SDK client + options  
- system prompt (most important)  
- model  
- resume id  
- **all tools disallowed** (tool calling is useless for this specific use case)

**10:11**  
System prompt pattern: purpose, variables, instructions, examples.  
Bug noted: it originally used a hardcoded “3 versions”; then was updated to a variable “number of versions”.  
We pass number of versions into system prompt and **lock it in as the law**.

**10:39**  
Instructions: respond JSON variations.  
Examples define output structure:
- primary response  
- multi-version copy responses

**11:03**  
Key idea: the agent should see all variations in its context window so it can generate them properly in one pass—otherwise you’d coordinate multiple agents.

**11:42**  
If responding with JSON: define a concrete schema/object (Python: **Pydantic**, TS: **Zod**).  
Then parse/process and return at the right time.

**12:11**  
Agents deployed in UIs as backend API methods:
- e.g., endpoint like `/copy` returns multiple variations  
Flow remains: options → SDK → query → manage responses (assistant block, result block, session id/cost).

---

## Custom Micro SDLC (plan → build → review → ship) in a UI (multi-agent orchestration)

**12:54**  
Now: custom micro SDLC agent. Codebase structure: backend, frontend, reviews, specs, tests.

**13:12**  
UI shows an agent-by-agent task board: hand off work from one agent to another in a micro SDLC.  
Tasks move across: plan, build, review, ship.

**13:29**  
Create a task: update HTML title and header title to “plan build review and ship agents”.  
Drag task into “plan” → workflow kicks off.

**14:03**  
Multi-agent system: planner agent starts (thinking, tool calls, hook interceptions, permission checks).  
Then builder agent runs.  
UI streams messages via websockets; tool calls counted separately.

**15:16**  
Spec file appears; workflow moves into review.  
Reviewer agent runs with its tools; eventually task moves to shipped.

**16:05**  
Result: both titles updated; shipped from a single out-of-loop prompt.  
This demonstrates out-of-loop review systems and TAC’s progression: in-loop → out-of-loop → ZTE.

---

### Micro SDLC internals: orchestrator + hooks + prompts separation

**16:57**  
Agent orchestrator is key. Search for `cloud_code_options`.  
Hooks limit planner writes:
- planner can only write to the `plan` directory  
- permission checks enforced via hooks

**17:39**  
Planner writes a spec in `specs/`.  
It’s a planning artifact; might be overkill, but provides a trail from planning → building → reviewing.

**18:10**  
Also produces a review file.

**18:31**  
Prompt organization:
- separate system prompts + user prompts as files  
- dedicated prompts directory  
- classic agentic prompt format: title, purpose, variables, instructions, workflow, report, etc.

**19:12**  
Builder agent: does not need a new system prompt; uses the base engineering agent.  
You don’t always need to overwrite system prompt.

**19:58**  
Reviewer agent appends to the default system prompt (doesn’t blow it away).  
Key idea: **extend** vs **fully custom** agents.

---

## Ultra Stream Agent (stream logs + summarization + inspector agent) + token-efficient chunk tools

**20:45**  
Final agent: Ultra Stream. Structure: backend, data, frontend, modules, system prompts, tests.

**21:03**  
Two agents:
- streaming agent (right): live feed of incoming logs  
- inspector agent (left): chat interface to investigate alerts

**21:16**  
Use case: ingest logs from backend / DataDog / GCP / AWS, filter important info.  
Just sitting in the UI → you receive alerts.  
Then ask inspector: “list issues 1–3, log summary only, and user.”

**21:49**  
Inspector calls domain-specific tools and outputs markdown:
- severity  
- summary  
- user responsible

**22:27**  
You can do actions: “alert support about issue #4” and keep processing new alerts as they appear.  
Two agents operate a stream: one summarizes/flags, the other investigates.

**23:20**  
Key concept: one agent can produce outputs that another agent consumes as input.  
Many multi-agent configurations remain untapped.

**23:37**  
All in one file (~400 lines) is not ideal for readability, but headers help.  
Search for `cloud_code_options` again:
- these are full custom agents from the ground up  
- system prompt at the top is recommended  
- disallow all unnecessary tools  
- use only domain-specific tools

**24:19**  
Stream agent tools: read stream file, produce summarized log, create alert.  
It processes continuously.

**24:35**  
Problem: logs can exceed context window.  
Example stream file is ~42K tokens and can grow much larger.  
So we build fine-grain tools:
- read logs in chunks by start/end line indexes  
- avoid overwhelming the model  
- reduce time/tokens/cost

**25:29**  
We process small windows (e.g., 35–40, 40–45, 45–50) rather than everything.  
Implementation uses deterministic code (subprocess) to fetch just the required chunk.  
This is token-efficient domain tooling.

**26:19**  
System prompt includes static variables (and can manage context reset via session id).  
Uses workflow + instructions + variables + examples to produce consistent log formats.

**27:14**  
Inspector agent:
- reads produced log entries by index  
- can find logs for a specific user  
- supports tracing before an error

**29:12**  
This is how you deploy custom agents: notify support/engineering, stack workflows, wrap in domain UI.

---

## Cuándo usar custom agents vs out-of-the-box (extender vs reemplazar)

**29:38**  
You don’t always need custom agents.  
Micro SDLC example extended Cloud Code via SDK; did not fully replace it.

**29:52**  
Build custom agents when you need one or more:
- programmatic agents  
- repeat workflows  
- solve domain problems better than generic agents  
- keep costs down while keeping performance up  
- permission checks (hooks/governance) to prevent damage  
- stay out of the loop (core TAC idea)

**31:04**  
Use out-of-the-box agents when:
- you’re operating in-loop (prompting back and forth)  
- workflow is generic enough  
- you’re exploring/prototyping  
- you need a balanced generalist engineering agent (works well in ~80%)  
- tasks are short-lived/lightweight/not repeatable/not domain-specific

**32:38**  
Distinction:
- **extended agents**: append system prompt, hide a few tools, small tweaks  
- **complete custom agents**: overwrite system prompt + custom tools

**33:12**  
Ultra Stream is not Cloud Code “product” at all (only uses SDK harness benefits like agent loop + prompt caching).  
Micro SDLC used Cloud Code base tools because it’s software work.

**34:08**  
Default strategy:
- use out-of-the-box as much as possible  
- only go full custom when you need unique prompts/models/hooks/tools/permissions tailored to your domain  
- as you scale: better agents → more agents → custom agents

---

## Wrap-up (Agentic Horizon + learning path)

**35:38**  
You get to vote on upcoming lessons.  
After enough votes, the next lesson becomes a focused deep dive on what engineers choose.

**36:37**  
Codebase will be available; all agents in `apps/`.  
From simple scripts → complex multi-agent UIs to control compute with custom agents.

**37:06**  
Deploying effective compute/agents is about finding constraints in your workflow/products:  
repeat workflows → agents in scripts, streams, terminals, UIs → high ROI problems.

**37:29**  
Agentic coding is less about what we can do and more about what we can teach our agents to do.  
Follow the path: **better agents, more agents, then custom agents**.